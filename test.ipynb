{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GCS to BigQuery Data Ingestion Pipeline\n",
        "\n",
        "Created: 2026-01-20\n",
        "\n",
        "This notebook contains Spark code for ingesting payment data from Google Cloud Storage (GCS) to BigQuery.\n",
        "\n",
        "## Configuration\n",
        "- **Source**: Google Cloud Storage (GCS)\n",
        "- **Destination**: BigQuery (prd-dagen.payments_v1)\n",
        "- **Data Format**: Parquet (configurable)\n",
        "- **Processing**: PySpark with DataFrame API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(f\"Starting GCS to BigQuery ingestion pipeline at {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session with BigQuery connector\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"gcs-to-bigquery-payments\") \\\n",
        "    .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.32.1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set Spark logging level\n",
        "spark.sparkContext.setLogLevel(\"INFO\")\n",
        "\n",
        "logger.info(\"Spark session initialized successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "GCS_BUCKET = \"gs://your-gcs-bucket\"  # Replace with your GCS bucket\n",
        "GCS_PATH = f\"{GCS_BUCKET}/payments-data\"  # Path to payment data in GCS\n",
        "DATA_FORMAT = \"parquet\"  # Can be: parquet, csv, json, orc\n",
        "\n",
        "# BigQuery Configuration\n",
        "BQ_PROJECT = \"prd-dagen\"\n",
        "BQ_DATASET = \"payments_v1\"\n",
        "BQ_TABLE = \"payments_raw\"  # Replace with your target table\n",
        "BQ_TEMP_BUCKET = \"gs://your-temp-bucket\"  # Temporary bucket for BigQuery operations\n",
        "\n",
        "logger.info(f\"Configuration: GCS_PATH={GCS_PATH}, BQ_TABLE={BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read data from GCS\n",
        "try:\n",
        "    logger.info(f\"Reading {DATA_FORMAT.upper()} data from GCS: {GCS_PATH}\")\n",
        "    \n",
        "    if DATA_FORMAT.lower() == \"parquet\":\n",
        "        df = spark.read.parquet(GCS_PATH)\n",
        "    elif DATA_FORMAT.lower() == \"csv\":\n",
        "        df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(GCS_PATH)\n",
        "    elif DATA_FORMAT.lower() == \"json\":\n",
        "        df = spark.read.json(GCS_PATH)\n",
        "    elif DATA_FORMAT.lower() == \"orc\":\n",
        "        df = spark.read.orc(GCS_PATH)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported data format: {DATA_FORMAT}\")\n",
        "    \n",
        "    logger.info(f\"Successfully read data from GCS. Row count: {df.count()}\")\n",
        "    logger.info(f\"Schema: {df.schema}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Error reading from GCS: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data validation and cleaning\n",
        "try:\n",
        "    logger.info(\"Starting data validation and cleaning\")\n",
        "    \n",
        "    # Add ingestion timestamp\n",
        "    df = df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
        "    \n",
        "    # Handle null values (example - adjust based on your data)\n",
        "    # df = df.fillna(0, subset=[\"amount\"])\n",
        "    # df = df.fillna(\"UNKNOWN\", subset=[\"category\"])\n",
        "    \n",
        "    # Remove duplicates if needed\n",
        "    # df = df.dropDuplicates()\n",
        "    \n",
        "    logger.info(f\"Data validation complete. Final row count: {df.count()}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during data validation: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample data\n",
        "logger.info(\"Sample of processed data:\")\n",
        "df.show(5, truncate=False)\n",
        "print(f\"\\nTotal Rows: {df.count()}\")\n",
        "print(f\"Schema:\\n{df.printSchema()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write data to BigQuery\n",
        "try:\n",
        "    logger.info(f\"Writing data to BigQuery: {BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}\")\n",
        "    \n",
        "    df.write \\\n",
        "        .format(\"bigquery\") \\\n",
        "        .option(\"table\", f\"{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}\") \\\n",
        "        .option(\"temporaryGcsBucket\", BQ_TEMP_BUCKET) \\\n",
        "        .option(\"writeMethod\", \"direct\") \\\n",
        "        .mode(\"append\") \\\n",
        "        .save()\n",
        "    \n",
        "    logger.info(f\"Successfully wrote {df.count()} rows to BigQuery\")\n",
        "    print(f\"✓ Data successfully ingested to BigQuery table: {BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Error writing to BigQuery: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verification: Query BigQuery to confirm data was written\n",
        "try:\n",
        "    logger.info(\"Verifying data in BigQuery\")\n",
        "    \n",
        "    verification_df = spark.read \\\n",
        "        .format(\"bigquery\") \\\n",
        "        .option(\"table\", f\"{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}\") \\\n",
        "        .load()\n",
        "    \n",
        "    row_count = verification_df.count()\n",
        "    logger.info(f\"Verification successful. Total rows in BigQuery table: {row_count}\")\n",
        "    print(f\"✓ Verification complete. Table contains {row_count} rows.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during verification: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "logger.info(f\"Ingestion pipeline completed at {datetime.now()}\")\n",
        "spark.stop()\n",
        "logger.info(\"Spark session stopped\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}